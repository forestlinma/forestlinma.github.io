<!DOCTYPE html>
<!-- saved from url=(0031)https://jd92.wang/publications/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Lin Ma


  | Publications

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="./Publications_files/bootstrap.min.css" rel="stylesheet">
<link rel="stylesheet" href="./Publications_files/mdb.min.css">

<!-- Fonts & Icons -->
<link rel="stylesheet" href="./Publications_files/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="./Publications_files/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="./Publications_files/css">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%F0%9F%90%BC&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="./Publications_files/main.css">
<link rel="canonical" href="https://jd92.wang/publications/jindongwang.github.io/publications/">



  <style type="text/css">.medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--opened .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s cubic-bezier(.2,0,.2,1)!important}.medium-zoom-image--hidden{visibility:hidden}.medium-zoom-image--opened{position:relative;cursor:pointer;cursor:zoom-out;will-change:transform}</style><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}
</style></head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>
    
    <script src="./Publications_files/hm.js.下载"></script><script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?29274d0e68415cd4179a801ea4abf7cb";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://jd92.wang/">
       <span class="font-weight-bold">Lin Ma</span>
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://jd92.wang/tlbook/">
                Home
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://jd92.wang/publications/">
                News
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://jd92.wang/research/">
                Research
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://jd92.wang/service/">
                <strong>Publications</strong>
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://jd92.wang/students/">
                Awards
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="https://jd92.wang/talks/">
                Services
                
              </a>
          </li>
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">


<div class="news">
    <p><strong>Publications</strong></p>

    <p><strong>2023</strong></p>


    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tbody>


     

          <tr>
          <th width="2%" scope="row" align="left" valign="top">&bull;</th>
          <td width="98%" align="left">
            <strong>E2E-LOAD: End-to-End Long-form Online Action Detection</strong><br>
Shuqiang Cao, Weixin Luo, Bairui Wang, Wei Zhang, and <strong>Lin Ma</strong> <br>
International Conference on Computer Vision (<strong>ICCV</strong>), 2023. <br>
          </td>
        </tr>


        <tr>
          <th scope="row" valign="top">&bull;</th>
          <td>
           <strong>Zero-Shot Semantic Segmentation with Decoupled One-Shot Network</strong><br>
Cong Han, Yujie Zhong, Kai Han, Dengjie Li, and <strong>Lin Ma</strong><br>
International Conference on Computer Vision (<strong>ICCV</strong>), 2023. <br>
          </td>
        </tr>




         <tr>
          <th scope="row" valign="top">&bull;</th>
          <td>
           <strong>A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text</strong><br>
Yunxin Li, Baotian Hu, Yuxin Ding, <strong>Lin Ma</strong>, and Min Zhang<br>
The 61st Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>), 2023.<br>
          </td>
        </tr>



         <tr>
          <th scope="row" valign="top">&bull;</th>
          <td>
           <strong>A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues</strong><br>
Yunxin Li, Baotian Hu, Xinyu Chen,  Yuxin Ding, <strong>Lin Ma</strong>, and Min Zhang<br>
The 61st Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>), 2023.<br>
          </td>
        </tr>




         <tr>
          <th scope="row" valign="top">&bull;</th>
          <td>
           <strong>AeDet: Azimuth-Invariant Multi-View 3D Object Detection</strong><br>
Chengjian Feng, Zequn Jie, Yujie Zhong, Xiangxiang Chu, and <strong>Lin Ma</strong><br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <br>
          </td>
        </tr>



         <tr>
          <th scope="row" valign="top">&bull;</th>
          <td>
           <strong>Adaptive Sparse Pairwise Loss for Object Re-Identification</strong><br>
Xiao Zhou, Yujie Zhong, Zhen Chen, Fan Liang, and <strong>Lin Ma</strong><br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <br>
          </td>
        </tr>
        



         <tr>
          <th scope="row" valign="top">&bull;</th>
          <td>
           <strong>MSMDFusion: Fusing LiDAR and Camera at Multiple Scales With Multi-Depth Seeds for 3D Object Detection</strong><br>
Yang Jiao, Zequn Jie, Shaoxiang Chen, Jingjing Chen, <strong>Lin Ma</strong>, and Yu-Gang Jiang <br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <br>
          </td>
        </tr>
        




         <tr>
          <th scope="row" valign="top">&bull;</th>
          <td>
           <strong>TriDet: Temporal Action Detection With Relative Boundary Modeling </strong><br>
Dingfeng Shi, Yujie Zhong, Qiong Cao, <strong>Lin Ma</strong>, Jia Li, and Dacheng Tao<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <br>
          </td>
        </tr>
        
 


        <tr>
          <th scope="row" valign="top">&bull;</th>
          <td>
           <strong>Curriculum Multi-Negative Augmentation for Debiased Video Grounding</strong><br>
Xiaohan Lan, Yitian Yuan, Hong Chen, Xin Wang, Zequn Jie, <strong>Lin Ma</strong>, Zhi Wang, and Wenwu Zhu<br>
The Thirty-seventh AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2023.<br>
          </td>
        </tr>





        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>Beyond Monocular Deraining: Parrallel Stereo Deraining Network via Semantic Prior</strong><br>
        Kaihao Zhang, Wenhan Luo, Yanjiang Yu, Wenqi Ren, Fang Zhao, Changsheng Li, <strong>Lin Ma</strong>, Wei Liu, and Hongdong Li
        <br>
        Internatinal Journal of Computer Vision (<strong>IJCV</strong>). Accepted <br>
        [<a href="https://arxiv.org/abs/2112.01062" target="_blank">arXiv Link</a>]
          </td>
        </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>Syntax Customized Video Captioning by Imitating Exemplar Sentences</strong><br>
        Yitian Yuan, <strong>Lin Ma</strong>, and Wenwu Zhu
        <br>
        IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>). Accepted <br>
        [<a href="https://arxiv.org/abs/2112.01062" target="_blank">arXiv Link</a>]


          </td>
        </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach</strong><br>
        Xiaohan Lan, Yitian Yuan, Xin Wang, Long Chen, Zhi Wang, <strong>Lin Ma</strong>, and Wenwu Zhu
        <br>
        ACM Transactions on Multimedia Computing Communications and Applications (<strong>TOMM</strong>). Accepted <br>


          </td>
        </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>Weakly Supervised Semantic Segmentation via Progressive Patch Learning</strong><br>
        Jinglong Li, Zequn Jie, Xu Wang, Yu Zhou, Xiaolin Wei, and <strong>Lin Ma</strong>
        <br>
        IEEE Transactions on Multimedia (<strong>TMM</strong>). Accepted <br>
        [<a href="https://arxiv.org/abs/2209.07828" target="_blank">arXiv Link</a>][<a href="https://github.com/TyroneLi/PPL_WSSS" target="_blank">Source Code</a>]


          </td>
        </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Fast and Robust Online Handwritten Chinese Character Recognition with Deep Spatial & Contextual Information Fusion Network</strong><br>
        Yunxin Li, Yunxin Li, Qian Yang, Qingcai Chen, Baotian Hu, Xiaolong Wang, Yuxin Ding, and <strong>Lin Ma</strong>
        <br>
        IEEE Transactions on Multimedia (<strong>TMM</strong>). Accepted <br>


          </td>
        </tr>







      </tbody>
    </table>
</div>






    <p><strong>2022</strong></p>


 <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tbody>
          <tr>
          <th width="2%" scope="row" align="left">&bull;</th>
          <td width="98%" align="left">
         <strong>Disentangled Feature Networks for Facial Portraits Generation</strong><br>
        Kaihao Zhang, Wenhan Luo, <strong>Lin Ma</strong>, Wenqi Ren, and Hongdong Li
        <br>
        IEEE Transactions on Multimedia (<strong>TMM</strong>), vol. 24, pp. 1378-1388, 2022.<br>
          </td>
        </tr>
      




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Expansion and Shrinkage of Localization for Weakly-Supervised Semantic Segmentation</strong><br>
Jinlong Li, Zequn Jie, Xu Wang, Xiaolin Wei, and <strong>Lin Ma</strong><br>
The Thirty-sixth Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022.<br>
[<a href="https://arxiv.org/abs/2209.07761" target="_blank">arXiv Link</a>][<a href="https://github.com/TyroneLi/ESOL_WSSS" target="_blank">Source Code</a>]

        
          </td>
        </tr>
      


        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos</strong><br>
        Yitian Yuan, <strong>Lin Ma</strong>, Jingwen Wang, Wei Liu, and Wenwu Zhu
        <br>
        IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), vol. 44, no. 5, pp. 2725-2741, May 2022. <br>

        
          </td>
        </tr>



 <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>Content-aware recommendation via Dynamic Heterogeneous Graph Convolutional Network</strong><br>
        Tingting Liang, <strong>Lin Ma</strong>, Weizhong Zhang, Haoran Xu, Congying Xia, and Yuyu Yin
        <br>
        Knowledge-Based Systems (<strong>KBS</strong>). Accepted <br>

        
          </td>
        </tr>

        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>Liquid Warping GAN with Attention: A Unified Framework for Human Image Synthesis</strong><br>
        Wen Liu, Zhixin Piao, Zhi Tu, Wenhan Luo, <strong>Lin Ma</strong>, and Shenghua Gao
        <br>
        IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>). Accepted <br>

        
          </td>
        </tr>






        <tr>
          <th scope="row">&bull;</th>
          <td>
            <strong>PromptDet: Expand Your Detector Vocabulary with Uncurated Images</strong><br>
Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, Xiaolin Wei, Weidi Xie, and <strong>Lin Ma</strong><br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2022. <br>
[<a href="https://arxiv.org/abs/2203.16513" target="_blank">arXiv Link</a>][<a href="https://fcjian.github.io/promptdet" target="_blank">Project Homepage</a>][<a href="https://github.com/fcjian/PromptDet" target="_blank">Source Code</a>]
          </td>
        </tr>
      
        <tr>
          <th scope="row">&bull;</th>
          <td>
            <strong>MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes</strong><br>
Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, <strong>Lin Ma</strong>, and Yu-Gang Jiang<br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2022. <br>
[<a href="https://arxiv.org/abs/2203.05203" target="_blank">arXiv Link</a>][<a href="https://github.com/SxJyJay/MORE" target="_blank">Source Code</a>]
          </td>
        </tr>


      
        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>ReAct: Temporal Action Detection with Relational Action Queries</strong><br>
Dingfeng Shi, Yujie Zhong, Qiong Cao, Jing Zhang, <strong>Lin Ma</strong>, Jia Li, and Dacheng Tao<br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2022. <br>
[<a href="https://arxiv.org/abs/2207.07097" target="_blank">arXiv Link</a>][<a href="https://github.com/sssste/React" target="_blank">Source Code</a>]
          </td>
        </tr>



     
        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>Contrastive Video-Language Learning with Fine-grained Frame Sampling</strong><br>
Zixu Wang, Yujie Zhong, Yishu Miao, <strong>Lin Ma</strong>, and Lucia Specia<br>
The 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and <br> the 12th International Joint Conference on Natural Language Processing (<strong>AACL-IJCNLP</strong>), 2022. <br>
          </td>
        </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Cycle-Interactive Generative Adversarial Network for Robust Unsupervised Low-Light Enhancement</strong><br>
Zhangkai Ni, Wenhan Yang, Hanli Wang, Shiqi Wang, <strong>Lin Ma</strong>, and Sam Kwong<br>
The 30th ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022. <br>
[<a href="https://arxiv.org/abs/2207.00965" target="_blank">arXiv Link</a>][<a href="https://eezkni.github.io/publications/CIGAN.html" target="_blank">Project Homepage</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations</strong><br>
Qian Yang, Yunxin Li, Baotian Hu, <strong>Lin Ma</strong>, Yuxin Ding, and Min Zhang<br>
The 30th ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022. <br>
[<a href="https://arxiv.org/abs/2207.11401" target="_blank">arXiv Link</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Explore Inter-Contrast Between Videos via Composition for Weakly Supervised Temporal Sentence Grounding</strong><br>
Jiaming Chen, Weixin Luo, Wei Zhang, and <strong>Lin Ma</strong><br>
The Thirty-sixth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022. <br>
[<a href="https://www.aaai.org/AAAI22Papers/AAAI-2108.ChenJ.pdf" target="_blank">Full Text</a>]
          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Visual Consensus Modeling for Video-Text Retrieval</strong><br>
Shuqiang Cao, Bairui Wang, Wei Zhang, and <strong>Lin Ma</strong><br>
The Thirty-sixth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022. <br>
[<a href="https://www.aaai.org/AAAI22Papers/AAAI-12427.CaoS.pdf" target="_blank">Full Text</a>]
          </td>
          </tr>


      </tbody>
    </table>
</div>


 <p><strong>2021</strong></p>


    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tbody><tr>
          <th width="2%" scope="row" align="left">&bull;</th>
          <td width="98%" align="left">
          <strong>CASNet: A Cross-attention Siamese Network for Video Salient Object Detection</strong><br>
        Yuzhu Ji, Haijun Zhang, Zequn Jie, <strong>Lin Ma</strong>, and Jonathan Wu
        <br>
        IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>), vol. 32, no. 6, pp. 2676-2690, Jun. 2021. <br>
          </td>
          </tr>


 <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Progressive Point Cloud Upsampling via Differentiable Rendering</strong><br>
        Pingping Zhang, Xu Wang, <strong>Lin Ma</strong>, Shiqi Wang, Sam Kwong, and Jianmin Jiang
        <br>
        IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), vol. 31, no. 12, pp. 4673-4685, Dec. 2021. <br>
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Coupled Network for Robust Pedestrian Detection with Gated Multi-Layer Feature Extraction and Deformable Occlusion Handling</strong><br>
        Tianrui Liu, Wenhan Luo, <strong>Lin Ma</strong>, Jun-jie Huang, Tania Stathaki, and Tianhong Dai
        <br>
        IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 30, pp. 754-766, 2021.<br>
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Pyramid Global Context Network for Image Dehazing</strong><br>
        Dong Zhao, Long Xu, <strong>Lin Ma</strong>, Jia Li, and Yihua Yan
        <br>
        IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), vol. 31, no. 8, pp. 3037-3050, Aug. 2021.<br>
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>PFAN++: Bi-Directional Image-Text Retrieval with Position Focused Attention Network</strong><br>
        Yaxiong Wang, Hao Yang, Xiuxiu Bai, Xueming Qian, <strong>Lin Ma</strong>, Jing Lu, Biao Li, and Xin Fan
        <br>
        IEEE Transactions on Multimedia (<strong>TMM</strong>), vol. 23, pp. 3362-3376, Sept. 2021. <br>
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Unsupervised Text-to-Image Synthesis</strong><br>
        Yanlong Dong, Ying Zhang, <strong>Lin Ma</strong>, Zhi Wang, and Jiebo Luo
        <br>
        Pattern Recognition (<strong>PR</strong>), vol. 110, 107573, Feb. 2021. <br>
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
       <strong>Quality Evaluation for Image Retargeting with Instance Semantics</strong><br>
        Leida Li, Yixuan Li, Jinjian Wu, <strong>Lin Ma</strong>, and Yuming Fang
        <br>
        IEEE Transactions on Multimedia (<strong>TMM</strong>), vol. 23, pp. 2757-2769, 2021.<br>
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Two-stage Visual Cues Enhancement Network for Referring Image Segmentation</strong><br>
Yang Jiao, Zequn Jie, Weixin Luo, Jingjing Chen, Yu-Gang Jiang, Xiaolin Wei, and <strong>Lin Ma</strong><br>
The 29th ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2021. <br>
[<a href="https://arxiv.org/abs/2110.04435" target="_blank">arXiv Link</a>]
          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Cross-modality Discrepant Interaction Network for RGB-D Salient Object Detection</strong><br>
Chen Zhang, Runmin Cong, Qinwei Lin, <strong>Lin Ma</strong>, Feng Li, Yao Zhao, and Sam Kwong<br>
The 29th ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2021. <br>
[<a href="https://arxiv.org/abs/2108.01971" target="_blank">arXiv Link</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Relation-aware Instance Refinement for Weakly Supervised Visual Grounding</strong><br>
Yongfei Liu, Bo Wan, <strong>Lin Ma</strong>, and Xuming He<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021. <br>
[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Relation-aware_Instance_Refinement_for_Weakly_Supervised_Visual_Grounding_CVPR_2021_paper.pdf" target="_blank">Full Text</a>][<a href="https://github.com/youngfly11/ReIR-WeaklyGrounding.pytorch" target="_blank">Source Code</a>]
          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Neural Symbolic Representation Learning for Image Captioning</strong><br>
Xiaomei Wang, <strong>Lin Ma</strong>, Yanwei Fu, and Xiangyang Xue<br>
ACM International Conference on Multimedia Retrieval (<strong>ICMR</strong>), 2021. <br>
[<a href="https://dl.acm.org/doi/abs/10.1145/3460426.3463637" target="_blank">arXiv Link</a>]
          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Similarity Reasoning and Filtration for Image-Text Matching</strong><br>
Haiwen Diao, Ying Zhang, <strong>Lin Ma</strong>, and Huchuan Lu<br>
The Thirty-fifth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2021. <br>
[<a href="https://github.com/Paranioar/SGRAF" target="_blank">Source Code</a>]
          </td>
          </tr>





      </tbody>
    </table>
</div>




 <p><strong>2020</strong></p>


    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tbody><tr>
          <th width="2%" scope="row" align="left">&bull;</th>
          <td width="98%" align="left">
          <strong>Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning</strong><br>
        Wei Zhang, Bairui Wang, <strong>Lin Ma</strong>, and Wei Liu
        <br>
        IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), vol. 42, no. 12, pp. 3088-3101, Dec. 2020.<br>
          </td>
          </tr>

  <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Towards Unsupervised Deep Image Enhancement with Generative Adversarial Network</strong><br>
        Zhangkai Ni, Wenhan Yang, Shiqi Wang, <strong>Lin Ma</strong>, and Sam Kwong
        <br>IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 29, pp. 9140-9151, 2020. <br>
          </td>
          </tr>


          <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Multi-Exposure Decomposition-Fusion Model for High Dynamic Range Image Saliency Detection</strong><br>
        Xu Wang, Zhenhao Sun, Qiudan Zhang, Yuming Fang, <strong>Lin Ma</strong>, Shiqi Wang, and Sam Kwong
        <br>
        IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), vol. 30, no. 12, pp. 4409-4420, Dec. 2020.<br>
          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Matching Image and Sentence with Multi-faceted Representations</strong><br>
        <strong>Lin Ma</strong>, Wenhao Jiang, Zequn Jie, Yu-Gang Jiang, and Wei Liu
        <br>
        IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), vol. 30, no. 7, pp. 2250-2261, Jul. 2020.  <br>
          </td>
          </tr>





        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Controllable Video Captioning with an Exemplar Sentence</strong><br>
Yitian Yuan, <strong>Lin Ma</strong>, Jingwen Wang, and Wenwu Zhu<br>
The 28th ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2020. <br>
          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Every Moment Matters: Detail-Aware Networks to Bring a Blurry Image Alive</strong><br>
Kaihao Zhang, Wenhan Luo, Bjorn Stenger, Wenqi Ren, <strong>Lin Ma</strong>, and Hongdong Li<br>
The 28th ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2020. <br>
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Context-Gated Convolution</strong><br>
Xudong Lin, <strong>Lin Ma</strong>, Wei Liu, and Shih-Fu Chang<br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2020. <br>
[<a href="https://arxiv.org/abs/1910.05577" target="_blank">arXiv Link</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Beyond Monocular Deraining: Stereo Image Deraining via Semantic Understanding</strong><br>
Kaihao Zhang, Wenhan Luo, Wenqi Ren, Jingwen Wang, Fang Zhao, <strong>Lin Ma</strong>, and Hongdong Li<br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2020. <br>
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Consensus-Aware Visual-Semantic Embedding for Image-Text Matching</strong><br>
Haoran Wang, Ying Zhang, Zhong Ji, Yanwei Pang, and <strong>Lin Ma</strong><br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2020. <br>
[<a href="https://arxiv.org/abs/2007.08883" target="_blank">arXiv Link</a>][<a href="https://github.com/BruceW91/CVSE" target="_blank">Source Code</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension</strong><br>
Zhenfang Chen, Peng Wang, <strong>Lin Ma</strong>, Kwan-Yee Wong, and Qi Wu<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020. <br>
[<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Cops-Ref_A_New_Dataset_and_Task_on_Compositional_Referring_Expression_CVPR_2020_paper.pdf" target="_blank">Full Text</a>][<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Chen_Cops-Ref_A_New_CVPR_2020_supplemental.pdf" target="_blank">Supplementary Material</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Deblurring by Realistic Blurring</strong><br>
Kaihao Zhang, Wenhan Luo, Yiran Zhong, <strong>Lin Ma</strong>, Bjorn Stenger, Wei Liu, and Hongdong Li<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020. <br>
[<a href="hhttps://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Deblurring_by_Realistic_Blurring_CVPR_2020_paper.pdf" target="_blank">Full Text</a>]
          </td>
          </tr>




          
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Fine-grained Image-to-Image Transformation towards Visual Recognition</strong><br>
Wei Xiong, Yutong He, Yixuan Zhang, Wenhan Luo, <strong>Lin Ma</strong>, and Jiebo Luo<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020. <br>
[<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xiong_Fine-Grained_Image-to-Image_Transformation_Towards_Visual_Recognition_CVPR_2020_paper.pdf">Full Text</a>][<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Xiong_Fine-Grained_Image-to-Image_Transformation_CVPR_2020_supplemental.pdf" target="_blank">Supplementary Material</a>][<a href="https://wxiong.me/finegrain/" target="_blank">Project Homepage</a>]
          </td>
          </tr>


   
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction</strong><br>
Jingwen Wang, <strong>Lin Ma</strong>, and Wenhao Jiang<br>
The Thirty-fourth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2020. <br>
[<a href="https://aaai.org/Papers/AAAI/2020GB/AAAI-WangJ.3705.pdf" target="_blank">Full Text</a>][<a href="https://github.com/JaywongWang/CBP" target="_blank">Source Code</a>]
          </td>
          </tr>



   
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Recurrent Nested Model for Sequence Generation</strong><br>
Wenhao Jiang, <strong>Lin Ma</strong>, and Wei Lu<br>
The Thirty-fourth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2020. <br>
[<a href="https://aaai.org/Papers/AAAI/2020GB/AAAI-JiangW.%203711.pdf" target="_blank">Full Text</a>]
          </td>
          </tr>

  
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Feature Deformation Meta-Networks in Image Captioning of Novel Objects</strong><br>
Tingjia Cao, Ke Han, Xiaomei Wang, <strong>Lin Ma</strong>, Yanwei Fu, Yu-Gang Jiang, and Xiangyang Xue<br>
The Thirty-fourth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2020. <br>
[<a href="https://aaai.org/Papers/AAAI/2020GB/AAAI-CaoT.4566.pdf" target="_blank">Full Text</a>]
</td>
          </tr>



 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Grasp for Stacking via Deep Reinforcement Learning</strong><br>
Junhao Zhang, Wei Zhang, Ran Song, <strong>Lin Ma</strong>, and Yibin Li<br>
International Conference on Robotics and Automation (<strong>ICRA</strong>), 2020.
          </td>
          </tr>



 
    


      </tbody>
    </table>
</div>








 <p><strong>2019</strong></p>


    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tbody><tr>
          <th width="2%" scope="row" align="left">&bull;</th>
          <td width="98%" align="left">
          <strong>Low-Light Image Enhancement via a Deep Hybrid Network</strong><br>
        Wenqi Ren, Sifei Liu, <strong>Lin Ma</strong>, Qianqian Xu, Xiangyu Xu, Xiaochun Cao, Junping Du, and Ming-Hsuan Yang
        <br>
        IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 28, no. 9, pp. 4364-4375, Sept. 2019. <br>
          </td>
          </tr>






        <tr>
          <th scope="row">&bull;</th>
          <td>
       <strong>Deep Video Dehazing with Semantic Segmentation</strong><br>
        Wenqi Ren, Jingang Zhang, Xiangyu Xu, <strong>Lin Ma</strong>, Xiaochun Cao, Gaofeng Meng, and Wei Liu
        <br>
        IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 28, no. 4, pp. 1895-1908, Apr. 2019. <br>
        <a href="welcome_files/Wenqi_Ren_Deep_Video_Dehazing_TIP_2019.pdf">[Full Text</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
       <strong>Bidirectional Image-Sentence Retrieval by Local and Global Deep Matching</strong><br>
        <strong>Lin Ma</strong>, Wenhao Jiang, Zequn Jie, and Xu Wang
        <br>
        Neurocomputing (<strong>NC</strong>), vol. 345, pp. 36-44, Jun. 2019. <br>
          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
       <strong>Towards Efficient Action Recognition: Principal Backpropagation for Training Two-Stream Networks</strong><br>
        Wenbing Huang, Lijie Fan, Mehrtash Harandi, Chuang Gan, <strong>Lin Ma</strong>, Huaping Liu, and Wei Liu
        <br>
        IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 28, no. 4, pp. 1773-1782, Apr. 2019. <br>
        <a href="welcome_files/Wenbing_Huang_Towards_Efficient_Action_Recognition_TIP_2019.pdf">[Full Text</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
       <strong>Adversarial Spatio-Temporal Learning for Video Deblurring</strong><br>
        Kaihao Zhang, Wenhan Luo, Yiran Zhong, <strong>Lin Ma</strong>, Wei Liu, and Hongdong Li
        <br>
        IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 28, no. 1, pp. 291-301, Jan. 2019. <br>
        <a href="welcome_files/Kaihao_Zhang_Adversarial_Spatiao-Temporal_Learning_TIP_2018.pdf">[Full Text</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Semantic Conditioned Dynamtic Modulation for Temporal Sentence Grounding in Videos</strong><br>
Yitian Yuan, <strong>Lin Ma</strong>, Jingwen Wang, Wei Liu, and Wenwu Zhu<br>
The Thirty-third Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2019. <br>
[<a href="https://papers.nips.cc/paper/8344-semantic-conditioned-dynamic-modulation-for-temporal-sentence-grounding-in-videos.pdf" target="_blank">Full Text</a>][<a href="https://github.com/yytzsy/SCDM" target="_blank">Source Code</a>]
          </td>
          </tr>





        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representation</strong><br>
Xu Wang, Jingming He, and <strong>Lin Ma</strong><br>
The Thirty-third Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2019.<br>
[<a href="https://papers.nips.cc/paper/8706-exploiting-local-and-global-structure-for-point-cloud-semantic-segmentation-with-contextual-point-representations.pdf">Full Text</a>][<a href="https://github.com/fly519/ELGS">Source Code</a>] 
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Controllable Video Captioning with POS Sequence Guidance Based on Gated Fusion Network</strong><br>
Bairui Wang, <strong>Lin Ma</strong>, Wei Zhang, Wenhao Jiang, Jingwen Wang, and Wei Liu<br>
International Conference on Computer Vision (<strong>ICCV</strong>), 2019. <br>
[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Controllable_Video_Captioning_With_POS_Sequence_Guidance_Based_on_Gated_ICCV_2019_paper.pdf">Full Text</a>][<a href="https://arxiv.org/pdf/1908.10072.pdf">arXiv Link</a>][<a href="https://github.com/vsislab/Controllable_XGating">Source Code</a>] 
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis</strong><br>
Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, <strong>Lin Ma</strong>, Shenghua Gao<br>
International Conference on Computer Vision (<strong>ICCV</strong>), 2019. <br>
[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Liquid_Warping_GAN_A_Unified_Framework_for_Human_Motion_Imitation_ICCV_2019_paper.pdf">Full Text</a>][<a href="https://arxiv.org/pdf/1909.12224.pdf">arXiv Link</a>][<a href="https://svip-lab.github.io/project/impersonator.html">Project Homepage</a>][<a href="https://github.com/svip-lab/impersonator">Source Code</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Sentence Specified Dynamic Video Thumbnail Generation</strong><br>
Yitian Yuan, <strong>Lin Ma</strong>, and Wenwu Zhu<br>
The 27th ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2019.<br>
[<a href="https://arxiv.org/pdf/1908.04052.pdf">arXiv Link</a>][<a href="https://github.com/yytzsy/GTP">Source Code</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video</strong><br>
Zhenfang Chen, <strong>Lin Ma</strong>, Wenhan Luo, and Kwan-Yee K. Wong<br>
The 57th Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>), 2019.<br>
[<a href="https://www.aclweb.org/anthology/P19-1183">Full Text</a>][<a href="https://arxiv.org/abs/1906.02549">arXiv Link</a>][<a href="https://github.com/JeffCHEN2017/WSSTG.git">Source Code</a>]  
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Hallucinating Optical Flow Features for Video Classification</strong><br>
Yongyi Tang, <strong>Lin Ma</strong>, and Lianqiang Zhou<br>
International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2019.<br>
[<a href="https://arxiv.org/abs/1905.11799">arXiv Link</a>][<a href="https://github.com/YongyiTang92/MoNet-Features">Source Code</a>] 
          </td>
          </tr>




          
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Position Focused Attention Network For Image-Text Matching</strong><br>
Yaxiong Wang, Hao Yang, Xueming Qian, <strong>Lin Ma</strong>, Jing Lu, Biao Li, and Xin Fan<br>
International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2019.<br>
[<a href="https://arxiv.org/pdf/1907.09748.pdf">arXiv Link</a>][<a href="https://github.com/HaoYang0123/Position-Focused-Attention-Network">Source Code</a>] 
          </td>
          </tr>


   
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Unsupervised Image Captioning</strong><br>
Yang Feng, <strong>Lin Ma</strong>, Wei Liu, and Jiebo Luo<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019.<br>
[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Unsupervised_Image_Captioning_CVPR_2019_paper.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1811.10787">arXiv Link</a>][<a href="https://github.com/fengyang0317/unsupervised_captioning">Source Code</a>] 
          </td>
          </tr>



   
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Spatio-temporal Video Re-localization by Warp LSTM</strong><br>
Yang Feng, <strong>Lin Ma</strong>, Wei Liu, and Jiebo Luo<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019.<br>
[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Spatio-Temporal_Video_Re-Localization_by_Warp_LSTM_CVPR_2019_paper.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1905.03922">arXiv Link</a>][<a href="https://github.com/fengyang0317/STVR">Source Code</a>]
          </td>
          </tr>

  
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Multi-granularity Generator for Temporal Action Proposal</strong><br>
Yuan Liu, <strong>Lin Ma</strong>, Yifeng Zhang, Wei Liu, and Shih-Fu Chang<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019.<br>
[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Multi-Granularity_Generator_for_Temporal_Action_Proposal_CVPR_2019_paper.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1811.11524">arXiv Link</a>] 
          </td>
          </tr>



 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Learning Joint Gait Representation via Quintuplet Loss Minimization</strong><br>
Kaihao Zhang, Wenhan Luo, <strong>Lin Ma</strong>, Wei Liu, and Hongdong Li<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019. (<strong>Oral</strong>)<br>
[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Learning_Joint_Gait_Representation_via_Quintuplet_Loss_Minimization_CVPR_2019_paper.pdf">Full Text</a>]
          </td>
          </tr>



 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Image Deformation Meta-Network for One-Shot Learning</strong><br>
Zitian Chen, Yanwei Fu, Yu-Xiong Wang, <strong>Lin Ma</strong>, Wei Liu, and Martial Hebert<br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019. (<strong>Oral</strong>)<br>
[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Image_Deformation_Meta-Networks_for_One-Shot_Learning_CVPR_2019_paper.pdf">Full Text</a>][<a href="https://github.com/tankche1/IDeMe-Net">Source Code</a>]
          </td>
          </tr>

 

 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Localizing Natural Language in Videos</strong><br>
Jingyuan Chen, <strong>Lin Ma</strong>, Xinpeng Chen, Zequn Jie, and Jiebo Luo<br>
The Thirty-third AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2019. <br>
[<a href="https://forestlinma.com/welcome_files/Jingyuan_Chen_Localizing_Natural_Language_In_Videos_AAAI_2019.pdf">Full Text</a>]
          </td>
          </tr>
    


 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Hierarchical Photo-Scene Encoder for Album Storytelling</strong><br>
Bairui Wang, <strong>Lin Ma</strong>, Wei Zhang, Wenhao Jiang, and Feng Zhang<br>
The Thirty-third AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2019. <br>
[<a href="https://forestlinma.com/welcome_files/Bairui_Wang_Hierarchical_Photo-Scene_Encoder_AAAI_2019.pdf">Full Text</a>]
          </td>
          </tr>
    


 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Cousin Network Guided Sketch Recognition via Latent Attribute Warehouse</strong><br>
Kaihao Zhang, Wenhan Luo, <strong>Lin Ma</strong>, and Hongdong Li<br>
The Thirty-third AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2019. <br>
[<a href="https://forestlinma.com/welcome_files/Kaihao_Zhang_Cousin_Network_Guided_Sketch_Recognition_AAAI_2019.pdf">Full Text</a>]
          </td>
          </tr>


      </tbody>
    </table>
</div>











 <p><strong>2018</strong></p>


    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tbody><tr>
          <th width="2%" scope="row" align="left">&bull;</th>
          <td width="98%" align="left">
            <strong>Deep Intensity Guidance Based Compression Artifacts Reduction for Depth Map</strong><br>
        Xu Wang, Pingping Zhang, Yun Zhang, <strong>Lin Ma</strong>, Sam Kwong, and Jianmin Jiang
        <br>
        Journal of Visual Communication and Image Representation, vol. 57, pp. 234-242, 2018. <br>
        <a href="welcome_files/Xu_Wang_Deep_Intensity_Guidance_JVCI_2018.pdf">[Full Text</a>]
         
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Screen Content Image Quality Assessment Using Multi-scale Difference of Gaussian</strong><br>
        Ying Fu, Huanqiang Zeng, <strong>Lin Ma</strong>, Zhangkai Ni, Canhui Cai, and Kai-Kuang Ma
        <br>
        IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), vol. 28, no. 9, pp. 2428-2432, Sept. 2018.  <br>
        <a href="welcome_files/Ying_Fu_Screen_Image_Quality_Assessment_TCSVT_2018.pdf">[Full Text</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>A Gabor Feature-based Quality Assessment Model for the Screen Content Images</strong><br>
        Zhangkai Ni, Huanqiang Zeng, <strong>Lin Ma</strong>,  Junhui Hou, Jing Chen, and Kai-Kuang Ma<br>
        IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 27, no. 9, pp. 4516-4528, Sept. 2018. <br>
        <a href="welcome_files/zkni_TIP_2018.pdf">[Full Text</a>] [<a href="http://smartviplab.org/pubilcations/GFM.html">Project Homepage</a>] 
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Image Processing for Synthesis Imaging of Mingantu Spectral Radioheliograph (MUSER)</strong><br>
        Long Xu, Yihua Yan, <strong>Lin Ma</strong>, and Yun Zhang<br>
        Multimedia Tools and Applications (<strong>MTAP</strong>), vol. 77, no. 16, pp. 20937-20954, Aug. 2018.<br>
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Reversible Data Hiding for High Dynamic Range Images Using Edge Information</strong><br>
        Xuanyu He, Wei Zhang, Haifeng Zhang, <strong>Lin Ma</strong>, Yibin Li<br>
        Multimedia Tools and Applications (<strong>MTAP</strong>). Accepted <br>
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Quaternion Represenation Based Visual Saliency for Stereoscopic Image Quality Assessment</strong><br>
        Xu Wang, <strong>Lin Ma</strong>, Sam Kwong, and Yu Zhou<br>
              Signal Processing (<strong>SP</strong>), vol. 145, pp. 202-213, Apr. 2018. <br>
                [<a href="welcome_files/xwang_sp_2018.pdf">Full Text</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Visual Tracking using Global Sparse Coding and Local Convolutional Features</strong><br>
        Xianyou Zeng, Long Xu, <strong>Lin Ma</strong>, Ruizhen Zhao, and Yigang Cen<br>
              Digital Signal Processing (<strong>DSP</strong>), vol. 72,  pp. 115-125, Jan. 2018. <br>
                [<a href="welcome_files/xyzeng_dsp2018.pdf">Full Text</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Deep Non-blind Deconvolution via Generalized Low-rank Approximation</strong><br>
Wenqi Ren, Jiawei Zhang, <strong>Lin Ma</strong>, Jinshan Pan, Xiaochun Cao, Wangmeng Zuo, Wei Liu, and Ming-Hsuan Yang<br>
The Thirty-second Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2018. <br>
[<a href="https://forestlinma.com/welcome_files/Wenqi_Ren_Deep_Non-Blind_Deconvolution_NIPS_2018.pdf">Full Text</a>][<a href="https://github.com/rwenqi/NBD-GLRA">Source Code</a>]
          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Parsimonious Quantile Regression of Asymmetrically Heavy-tailed Financial Return Series</strong><br>
Xing Yan, Weizhong Zhang, <strong>Lin Ma</strong>, Wei Liu, and Qi Wu<br>
The Thirty-second Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2018.<br>
[<a href="https://forestlinma.com/welcome_files/Xing_Yan_Parsimonious_Quantile_Regression_NIPS_2018.pdf">Full Text</a>] 
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Temporally Grounding Natural Sentence in Video</strong><br>
Jingyuan Chen, Xinpeng Chen, <strong>Lin Ma</strong>, Zequn Jie, and Tat-Seng Chua<br>
Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2018. <br>
[<a href="https://forestlinma.com/welcome_files/Jingyuan_Chen_Temporally_Grounding_EMNLP_2018.pdf">Full Text</a>][<a href="https://github.com/JaywongWang/TGN" target="_blank">Source Code</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Video Re-localization</strong><br>
Yang Feng, <strong>Lin Ma</strong>, Wei Liu, Tong Zhang, and Jiebo Luo<br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2018. <br>  
[<a href="https://forestlinma.com/welcome_files/Yang_Feng_Video_Re-localization_via_ECCV_2018_paper.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1808.01575">arXiv Link</a>][<a href="https://github.com/fengyang0317/video_reloc">Source Code</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Recurrent Fusion Network for Image Captioning</strong><br>
Wenhao Jiang, <strong>Lin Ma</strong>, Yu-Gang Jiang, Wei Liu, and Tong Zhang<br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2018.<br> 
[<a href="https://forestlinma.com/welcome_files/Wenhao_Jiang_Recurrent_Fusion_Network_ECCV_2018_paper.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1807.09986">arXiv Link</a>][<a href="https://github.com/cswhjiang/Recurrent_Fusion_Network">Source Code</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Unsupervised Image-to-Image Translation with Stacked Cycle-Consistent Adversarial Networks</strong><br>
Minjun Li, Haozhi Huang, <strong>Lin Ma</strong>, Wei Liu, Tong Zhang, and Yu-Gang Jiang<br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2018. <br>
[<a href="https://forestlinma.com/welcome_files/Minjun_Li_Unsupervised_Image-to-Image_Translation_ECCV_2018_paper.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1807.08536">arXiv Link</a>]  
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Neural Stereoscopic Image Style Transfer</strong><br>
Xinyu Gong, Haozhi Huang, <strong>Lin Ma</strong>, Fumin Shen, Wei Liu, and Tong Zhang<br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2018.<br>
[<a href="https://forestlinma.com/welcome_files/Xinyu_Gong_Neural_Stereoscopic_Image_ECCV_2018_paper.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1802.09985">arXiv Link</a>] 
          </td>
          </tr>




          
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Safe Element Screening for Submodular Function Minimization</strong><br>
      Weizhong Zhang, Bin Hong, <strong>Lin Ma</strong>, Wei Liu, and Tong Zhang<br>
      International Conference on Machine Learning (<strong>ICML</strong>), 2018. <br>
[<a href="https://forestlinma.com/welcome_files/weizhongzhang_ICML2018.pdf">Full Text</a>] 
          </td>
          </tr>


   
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Long-Term Human Motion Prediction by Modeling Motion Context and Enhancing Motion Dynamics</strong><br>
Yongyi Tang, <strong>Lin Ma</strong>, Wei Liu, and Wei-Shi Zheng<br>
International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2018. <br>
[<a href="https://forestlinma.com/welcome_files/Yongyi_Tang_Long-term_Human_Motion_Prediction_via_IJCAI_2018.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1805.02513">arXiv Link</a>] 
          </td>
          </tr>



   
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Image-level to Pixel-wise Labeling: From Theory to Practice</strong><br>
      Tiezhu Sun, Wei Zhang, Zhijie Wang, <strong>Lin Ma</strong>, and Zequn Jie      <br>
      International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2018.<br>
[<a href="https://forestlinma.com/welcome_files/Tiezhu_Sun_Image-leve_to_Pixel-wise_Labeling_via_IJCAI_2018.pdf">Full Text</a>]
          </td>
          </tr>

  
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present</strong><br>
        Xinpeng Chen, <strong>Lin Ma</strong>, Wenhao Jiang, Jian Yao, and Wei Liu<br>
        IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018. <br>
[<a href="https://forestlinma.com/welcome_files/xinpengchen_CVPR2018.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1803.11439">arXiv Link</a>][<a href="https://github.com/chenxinpeng/ARNet">Source Code</a>]
          </td>
          </tr>



 
        <tr>
          <th scope="row">&bull;</th>
          <td>
       <strong>Reconstruction Network for Video Captioning</strong><br>
      Bairui Wang, <strong>Lin Ma</strong>, Wei Zhang, and Wei Liu<br>  
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018. <br>
[<a href="https://forestlinma.com/welcome_files/bairuiwang_CVPR2018.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1803.11438">arXiv Link</a>]
          </td>
          </tr>



 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Gated Fusion Network for Single Image Dehazing</strong>        <br>
        Wenqi Ren, <strong>Lin Ma</strong>, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, and Ming-Hsuan Yang<br>
                IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018. <br>
[<a href="https://forestlinma.com/welcome_files/wenqiren_CVPR2018.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1804.00213">arXiv Link</a>][<a href="https://sites.google.com/site/renwenqi888/research/dehazing/gfn">Project Homepage</a>][<a href="https://github.com/rwenqi/GFN-dehazing">Source Code</a>]
          </td>
          </tr>

 

 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning</strong>  <br>
      Jingwen Wang, Wenhao Jiang, <strong>Lin Ma</strong>, Wei Liu, and Yong Xu<br>
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018. (<strong>Spotlight</strong>)<br>
[<a href="https://forestlinma.com/welcome_files/jingwenwang_CVPR2018.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1804.00100">arXiv Link</a>][<a href="https://github.com/JaywongWang/DenseVideoCaptioning">Source Code</a>]
          </td>
          </tr>
    


 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks</strong><br>
      Wei Xiong, Wenhan Luo, <strong>Lin Ma</strong>, Wei Liu, and Jiebo Luo<br>
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018. <br>
[<a href="https://forestlinma.com/welcome_files/weixiong_CVPR2018.pdf">Full Text</a>][<a href="https://arxiv.org/abs/1709.07592">arXiv Link</a>]
          </td>
          </tr>
    

    
 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Fine-grained Video Attractiveness Prediction Using Multimodal Deep Learning on a Large Real-world Dataset</strong><br>
      Xinpeng Chen, Jingyuan Chen, <strong>Lin Ma</strong>, Jian Yao, Wei Liu, Jiebo Luo, and Tong Zhang      <br>
      The Web Conference (original <strong>WWW</strong>), The Big Web Track, 2018.
       <br>
       [<a href="https://forestlinma.com/welcome_files/xpchen_WWW_2018.pdf">Ful Text</a>][<a href="https://arxiv.org/abs/1804.01373">arXiv Link</a>]  
          </td>
          </tr>


 
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Learning to Guide Decoding for Image Captioning</strong><br>
      Wenhao Jiang, <strong>Lin Ma</strong>, Xinpeng Chen, Hanwang Zhang, and Wei Liu      <br>
      The Thirty-second AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2018.
       <br>
[<a href="https://arxiv.org/abs/1804.00887">arXiv Link</a>]  
          </td>
          </tr>


      </tbody>
    </table>
</div>







 <p><strong>2017</strong></p>


    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tbody><tr>
          <th width="2%" scope="row" align="left">&bull;</th>
          <td width="98%" align="left">
          <strong>Real-Time Neural Style Transfer for Videos</strong><br>
      Haozhi Huang, Hao Wang, Wenhan Luo, <strong>Lin Ma</strong>, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu<br>
            IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2017. <br>
            [<a href="https://forestlinma.com/welcome_files/hzhuang_CVPR_2017.pdf">Full Text</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Objective Quality Assessment of Image Retargeting by Incorporating Fidelity Measures and Inconsistency Detection</strong><br>
        Yichi Zhang, King Ngi Ngan, <strong>Lin Ma</strong>,  and Hongliang Li<br>
              IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 26, no. 12, pp. 5980-5993, Dec. 2017. <br>
                <a href="welcome_files/yczhang_TIP_2017.pdf">[Full Text</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>ESIM: Edge Similarity for Screen Content Image Quality Assessment</strong><br>
        Zhangkai Ni, <strong>Lin Ma</strong>, Huanqiang Zeng, Jing Chen, Canhui Cai, and Kai-Kuang Ma<br> 
            IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 26, no. 10, pp. 4818-4831, Oct. 2017. <br>
            [<a href="welcome_files/lma_TIP2017_SCI.pdf">Full_Text</a>] 
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Multi-task Rank Learning for Image Quality Assessment</strong><br>
        Long Xu, Jia Li, Weisi Lin, Yongbing Zhang, <strong>Lin Ma</strong>, Yuming Fang, and Yihua Yan<br>
              IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), vol. 27, no. 9, pp. 1833-1843, Sept. 2017. <br>
                [<a href="welcome_files/lxu_TCSVT_2017.pdf">Full Text</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>The Use of Convolutional Neural Artificail Intellignece Network to Aid the Diagnosis and Classification of Early Esophageal Neoplasia</strong><br>
        Chenzi Zhang, <strong>Lin Ma</strong>, Noriya Uedo, Noriko Matsuura, Parry Tam, and Anthony Y. Teoh<br>
        Gastrointestinal Endoscopy, vol. 85, no. 5S, pp. AB507-AB588, 2017. <br>
        [<a href="welcome_files/czhang_GE2017.pdf">Full Text</a>]. 
          </td>
          </tr>

   
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Multimodal Deep Learning for Solar Radio Burst Classification<br>
        Lin Ma</strong>, Zhuo Chen, Long Xu, and Yihua Yan<em><br>
          </em>Pattern Recognition (<strong>PR</strong>), vol. 61, pp. 573-582, Jan. 2017. <br>
          [<a href="welcome_files/lma_PR_2017.pdf">Full Text</a>]
          </td>
          </tr>





        

      </tbody>
    </table>
</div>







 <p><strong>2016</strong></p>


    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tbody><tr>
          <th width="2%" scope="row" align="left">&bull;</th>
          <td width="98%" align="left">
         <strong>Learning to Answer Questions From Image Using Convolutional Neural Network</strong> <br>
      <strong>Lin Ma</strong>, Zhengdong Lu, and Hang Li      <br>
      The Thirtieth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2016. <br>
      [<a href="https://forestlinma.com/welcome_files/lma_AAAI2016_slides.pptx">Oral Presentation</a>][<a href="https://forestlinma.com/welcome_files/lma_AAAI_2016.pdf">Full Text</a>][<a href="http://arxiv.org/abs/1506.00333">arXiv Link</a>][<a href="http://conviqa.noahlab.com.hk/project.html">Project Homepage</a>] 
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Screen Content Image Quality Assessment Using Edge Model</strong><br>
      Zhangkai Ni, <strong>Lin Ma</strong>, Huanqiang Zeng, Canhui Cai, and Kai-Kuang Ma      <br>
      International Conference on Image Processing (<strong>ICIP</strong>), 2016. <br>
      [<a href="https://forestlinma.com/welcome_files/zkni_icip2016.pdf">Full Text</a>] 
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Perceptual Image Quality Enhancement for Solar Radio Image</strong><br>
      Long Xu,&nbsp;<strong>Lin Ma</strong>, Zhuo Chen, Xianyou Zeng, and Yihua Yan<br>
International Conference on Quality of Multimedia Experience (<strong>QoMex</strong>)<em>,</em> 2016.<br>
[<a href="http://www.ee.cuhk.edu.hk/~lma/welcome_files/lxu_qomex2016.pdf">Full Text</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Deep Learning Features Inspired Saliency Detection of 3D Images</strong><br>
      Qiudan Zhang, Xu Wang, Jianmin Jiang, and&nbsp;<strong>Lin Ma</strong><br>
      Pacific-Rim Conference on Multimedia (<strong>PCM</strong>), 2016. <br>
      [<a href="http://www.ee.cuhk.edu.hk/~lma/welcome_files/xwang_pcm_2016.pdf">Full Text</a>]abs/1808.01575">arXiv Link</a>][<a href="https://github.com/fengyang0317/video_reloc">Source Code</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>No-Reference Retargeted Image Quality Assessment Based on Pairwise Rank Learning<br>
        Lin Ma</strong>, Long Xu, Yichi Zhang, Yihua Yan, and King Ngi Ngan<br>
          IEEE Transaction on Multimedia (<strong>TMM</strong>), vol. 18, no. 11, pp. 2228-2237, Nov. 2016. <br>
            [<a href="welcome_files/lma_TMM_2016.pdf">Full Text</a>] 
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>Just Noticeable Difference Estimation for Screen Content Images</strong><br>
        Shiqi Wang, <strong>Lin Ma</strong>, Yuming Fang, Weisi Lin, Siwei Ma, and Wen Gao<br>
        IEEE Transactions on Image Processing (<strong>TIP</strong>), vol. 25, no. 8, pp. 3838-3851, Aug. 2016. <br>
        [<a href="welcome_files/sqwang_TIP_2016.pdf">Full Text</a>] 
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
           <strong>Gradient Direction for Screen Content Image Quality Assessment</strong><br>
        Zhangkai Ni, <strong>Lin Ma</strong>, Huanqiang Zeng, Canhui Cai, and Kai-Kuang Ma<br>
          IEEE Signal Processing Letters (<strong>SPL</strong>), vol. 23, no. 10, pp. 1394-1398, Oct. 2016. <br>
            [<a href="welcome_files/zkni_spl_2016.pdf">Full Text</a>] [<a href="http://www.ee.cuhk.edu.hk/~lma/welcome_files/SCI_GSS.html">Project Homepage</a>] [<a href="http://www.ee.cuhk.edu.hk/~lma/welcome_files/SCI_GSS/SCI_GSS.m">Code</a>]
          </td>
          </tr>


          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Learning Structure of Stereoscopic Image for No-Reference Quality Assessment with Convolutional Neural Network</strong><br>
        Wei Zhang, Chenfei Qu<strong>, Lin Ma</strong>, Jingwei Guan, and Rui Huang<br>
          Pattern Recognition (<strong>PR</strong>), vol. 59, pp. 176-187, Nov. 2016. <br>
          [<a href="welcome_files/lma_PR_2016.pdf">Full Text</a>] [<a href="http://www.ee.cuhk.edu.hk/~lma/welcome_files/StereoImageQA.html">Project Homepage</a>]
          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Free-energy Principle Inspired Video Quality Metric and Its Use in Video Coding</strong><br>
        Long Xu,  Weisi Lin, <strong>Lin Ma</strong>, Yongbing Zhang, Yuming Fang, King Ngi Ngan, Songnan Li, and Yihua Yan<br>
          IEEE Transaction on Multimedia (<strong>TMM</strong>), vol. 18, no. 4, pp. 590-602, Apr. 2016. <br>
          [<a href="welcome_files/lxu_TMM_2016.pdf">Full Text</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Reorganized DCT-based Image Representation for Reduced Reference Stereoscopic Image Quality Assessment<br>
        Lin Ma</strong>, Xu Wang, Qiong Liu, and King Ngi Ngan<br>
          Neurocomputing (<strong>NC</strong>), vol. 215, pp. 21-31, Nov. 2016. <br>
          [<a href="welcome_files/lma_NC_2016.pdf">Full Text</a>] 
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Imaging and Representation Learning of Solar Radio Spectrums for Classification</strong><br>
        Zhuo Chen<strong>, Lin Ma</strong>, Long Xu, Chengming Tan, and Yihua Yan<br>
          Multimedia Tools and Applications (<strong>MTAP</strong>), vol. 75, no. 5, pp. 2859-2875, Mar. 2016. <br>
            [<a href="welcome_files/lma_MTA_2015.pdf">Full Text</a>]
          </td>
          </tr>




        

      </tbody>
    </table>
</div>







 <p><strong>2015 and before</strong></p>


    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tbody><tr>
          <th width="2%" scope="row" align="left">&bull;</th>
          <td width="98%" align="left">
          <strong>Multimodal Convolutional Neural Networks for Matching Image and Sentence</strong><br>
      <strong>Lin Ma</strong>, Zhengdong Lu, Lifeng Shang, and Hang Li      <br>
      International Conference on Computer Vision (<strong>ICCV</strong>), 2015. <br>
      [<a href="https://forestlinma.com/welcome_files/ICCV2015_poster.pdf">Poster Presentation</a>][<a href="https://forestlinma.com/welcome_files/lma_ICCV_2015.pdf">Full Text</a>][<a href="http://arxiv.org/abs/1504.06063">arXiv Link</a>][<a href="http://mcnn.noahlab.com.hk/project.html">Project Homepage</a>]
          </td>
          </tr>





        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Multimodal Learning For Facial Expression Recognition</strong><br>
        Wei Zhang, Youmei Zhang<strong>, Lin Ma</strong>, Jingwei Guan, and Shijie Gong<br>
          Pattern Recognition (<strong>PR</strong>), vol. 48, no. 10, pp. 3191-3202, Oct. 2015. <br>
          [<a href="welcome_files/lma_PR_2015.pdf">Full Text</a>] [<a href="http://www.vsislab.com/papers/FER/FER.html">Project Homepage</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Reduced-Reference Image Quality Assessment in Reorganized DCT Domain<br>
        Lin Ma</strong>, Songnan Li, and King Ngi Ngan<br>
          Signal Processing: Image Communication (<strong>SPIC</strong>), vol. 28, no. 8, pp. 884-902, Aug. 2013. <br>
          [<a href="welcome_files/lma_SPIC_2013.pdf">Full Text</a>]. 
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Visual Saliency's Modulatory Effect on Just Noticeable Distortion Profile and Its Application in Image Watermarking</strong><br>
          Yaqing Niu, Matthew Kyan, <strong>Lin Ma</strong>, Azeddine Beghdadi, and Sridhar Krishnan<em><br>
            </em>Signal Processing: Image Communication (<strong>SPIC</strong>), vol. 28, no. 8, pp. 917-928, Aug. 2013. <br>
              [<a href="welcome_files/yaniu_SPIC_2013.pdf">Full Text</a>]. 
          </td>
          </tr>





        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Consistent Visual  Quality Control in Video Coding</strong><br>
        Long Xu, Songnan Li,  King Ngi Ngan, and <strong>Lin Ma</strong><br>
              IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), vol. 23, no. 6, pp. 975-989, Jun. 2013. <br>
            [<a href="welcome_files/lxu_TCSVT_2013.pdf">Full Text</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Recent Advances and Challenges of Visual Signal Quality Assessment<br>
        Lin Ma</strong>, Chenwei Deng, King Ngi Ngan, and Weisi Lin<br>
          China Communications, vol. 10, no. 5, pp. 62-78, 2013. <br>
          [<a href="welcome_files/lma_ChinaCommunications_2013.pdf">Full Text</a>] 
          </td>
          </tr>





        <tr>
          <th scope="row">&bull;</th>
          <td>
       <strong>Anaglyph Image Generation by Matching Color Appearance Attributes</strong><br>
        Songnan Li, <strong>Lin Ma</strong>, and King Ngi Ngan<br>
Signal Processing: Image Communication (<strong>SPIC</strong>), vol. 28, no. 6, pp. 597-607, Jul. 2013. <br>
[<a href="welcome_files/snli_SPIC.pdf">Full Text</a>] 
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
       <strong>Image Retargeting Quality Assessment: A Study of Subjective Scores and Objective Metrics<br>
        Lin Ma</strong>, Weisi Lin, Chenwei Deng, and King Ngi Ngan<br>
          IEEE Journal of Selected Topics in Signal Processing (<strong>JSTSP</strong>), vol. 6, no. 6, pp. 626-639, Oct. 2012. <br>
            [<a href="welcome_files/lma_JSTSP.pdf">Full Text </a>] [<a href="http://ivp.ee.cuhk.edu.hk/projects/demo/retargeting/index.html">Project Homepage</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
      <strong>Reduced-Reference Video Quality Assessment of Compressed Video Sequences<br>
        Lin Ma</strong>, Songnan Li, and King Ngi Ngan<br>
              IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), vol. 22, no. 10, pp. 1441-1456, Oct. 2012. <br>
                [<a href="welcome_files/lma_TCSVT_2012.pdf">Full Text</a>] 
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
     <strong>Full-reference Video Quality Assessment by Decoupling Detail Losses and Additive Impairments</strong><br>
        Songnan Li, <strong>Lin Ma</strong>, and King Ngi Ngan<br>
          IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), vol. 22, no. 7, pp. 1100-1112, Jul. 2012.  <br>
            [<a href="welcome_files/snli_TCSVT.pdf">Full Text</a>] 
          </td>
          </tr>





        <tr>
          <th scope="row">&bull;</th>
          <td>
    <strong>Learning-based Image Restoration for Compressed Images<br>
        Lin Ma</strong>, Debin Zhao, and Wen Gao<br>
        Signal Processing: Image Communication (<strong>SPIC</strong>), vol. 27, no. 1, pp. 54-65, Jan. 2012. <br>
        [<a href="welcome_files/lma_SPIC_2012.pdf">Full Text</a>] 
          </td>
          </tr>


  

        <tr>
          <th scope="row">&bull;</th>
          <td>
   <strong>Image Quality Assessment by Separately Evaluating Detail Losses and Additive Impairments</strong><br>
          Songnan Li, Fan Zhang, <strong>Lin Ma</strong>, and King Ngi Ngan<br>
          IEEE Transaction on Multimedia (<strong>TMM</strong>), vol. 13, no. 5, pp. 935-949, Oct. 2011. <br>
          [<a href="welcome_files/snli_TMM_2011.pdf">Full Text</a>] 
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
  <strong>Reduced-Reference Image Quality Assessment Using Reorganized DCT-Based Image Representation<br>
        Lin Ma</strong>, Songnan Li, Fan Zhang, and King Ngi Ngan<br>
        IEEE Transaction on Multimedia (<strong>TMM</strong>), vol. 13, no. 4, pp. 824-829, Aug. 2011. <br>
        [<a href="welcome_files/lma_TMM_2011.pdf">Full Text</a>] 
          </td>
          </tr>


 

        <tr>
          <th scope="row">&bull;</th>
          <td>
  <strong>Practical Image Quality Metric Applied to Image Coding</strong><br>
        Fan Zhang, <strong>Lin Ma</strong>, Songnan Li, and King Ngi Ngan<br>
              IEEE Transaction on Multimedia (<strong>TMM</strong>), vol. 13, no. 4, pp. 615-624, Aug. 2011. <br>
                [<a href="welcome_files/FanZhang_TMM_2011.pdf">Full Text</a>] [<a href="http://ivp.ee.cuhk.edu.hk/projects/demo/piqm/index.html">Experimental Results</a>]
          </td>
          </tr>





        <tr>
          <th scope="row">&bull;</th>
          <td>
  <strong>Adaptive Block-Size Transform Based Just-Noticeable Difference Model for Images/Videos<br>
        Lin Ma</strong>, King Ngi Ngan, Fan Zhang, and Songnan Li<br>
          Signal Processing: Image Communication (<strong>SPIC</strong>), vol. 26, no. 3, pp. 162-174, Mar. 2011. <br>
            [<a href="welcome_files/lma_SPIC_2011.pdf">Full Text</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
  <strong>Visual Horizontal Effect for Image Quality Assessment<br>
          Lin Ma</strong>, Songnan Li, and King Ngi Ngan<br>
            IEEE Signal Processing Letters (<strong>SPL</strong>), vol. 17, no. 7, pp. 627-630, Jul. 2010. <br>
            [<a href="welcome_files/lma_SPL_2010.pdf">Full Text</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
  <strong>Visual Signal Quality Assessment - Quality of Experience (QoE)<br>
        </strong>Chenwei Deng, <strong> Lin Ma</strong>, Weisi Lin, and King Ngi Ngan<br>
        Springer, ISBN: 978-3-319-10367-9, 303 pages, Nov. 2014. <br>
        [<a href="http://link.springer.com/book/10.1007%2F978-3-319-10368-6">Full Text</a>] 
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
  <strong>Retargeted Image Quality Assessment: Current Progresses  and Future Trends</strong><br>
        <strong>Lin Ma</strong>, Chenwei Deng, Weisi Lin, King Ngi Ngan, and Long Xu<br>
        Visual Signal Quality Assessment - Quality of Experience (QoE), Springer, ISBN: 978-3-319-10367-9, pp. 213-242, Nov. 2014. <br>
        [<a href="http://link.springer.com/chapter/10.1007/978-3-319-10368-6_8">Full Text</a>] 
          </td>
          </tr>


          

        <tr>
          <th scope="row">&bull;</th>
          <td>
  <strong>Conclusions and Perspectives</strong><br>
        Chenwei Deng, Shuigen Wang, and <strong>Lin Ma</strong><br>
        Visual Signal Quality Assessment - Quality of Experience (QoE), Springer, ISBN: 978-3-319-10367-9, pp. 287-302, Nov. 2014. <br>
        [<a href="http://link.springer.com/chapter/10.1007/978-3-319-10368-6_10">Full Text</a>] 
          </td>
          </tr>






        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>Perceptual Quality Improvement for Synthesis Imaging of Chinese Spectral Radiohelograph</strong><br>
      Long Xu,&nbsp;<strong>Lin Ma</strong>, Zhuo Chen, Yihua Yan, and Jinjian Wu<br>
      Pacific-Rim Conference on Multimedia (<strong>PCM</strong>), 2015.<br>
      [<a href="http://www.ee.cuhk.edu.hk/~lma/welcome_files/lxu_pcm2015.pdf">Full Text</a>]
          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>A Packet-Layer Model with Content Characteristics for Video Quality Assessment of IPTV</strong><br>
      Qian Zhang,&nbsp;<strong>Lin Ma</strong>, Fan Zhang, and Long Xu<br>
      Pacific-Rim Conference on Multimedia (<strong>PCM</strong>), 2015.<br>
      [<a href="http://www.ee.cuhk.edu.hk/~lma/welcome_files/qzhang_pcm2015.pdf">Full Text</a>]
          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Rank Learning Based No-Reference Quality Assessment of Retargeted Images<br>
        Lin Ma</strong>, Long Xu, Yichi Zhang, King Ngi Ngan, Yihua Yan<br>
        IEEE International Conference on Systems, Man, and Cybernetics (<strong>SMC</strong>), 2015.<br>
        [<a href="https://forestlinma.com/welcome_files/lma_smc2015.pdf">Full Text</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Multimodal Learning for Classification of Solar Radio Spectrum</strong><br>
      Zhuo Chen, <strong>Lin Ma</strong>, Long Xu, Yihua Yan<br>
        IEEE International Conference on Systems, Man, and Cybernetics (<strong>SMC</strong>) 2015.<br>
        [<a href="https://forestlinma.com/welcome_files/zchen_smc2015.pdf">Full Text</a>]
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
          <strong>Multi-task Rank Learning for Image Quality Assessment</strong><br>
      Long Xu, Jia Li, Weisi Lin, Yongbing Zhang, <strong>Lin Ma</strong>, Yuming Fang, Yun Zhang, and Yihua Yan<br>
        IEEE International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2015.<br>
        [<a href="https://forestlinma.com/welcome_files/lxu_icassp2015.pdf">Full Text</a>]  
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
         <strong>How Does the Shape Descriptor  Measure the Perceptual Quality of the Retargeted Image?<br>
      Lin Ma</strong>, Long Xu, Huanqiang  Zeng, King Ngi Ngan, and Chenwei Deng<br>
         IEEE International Conference on Multimedia  and Expo (<strong>ICME</strong>) Workshop on Emerging Multimedia Systems and Applications, 2014.<br>
           [<a href="https://forestlinma.com/welcome_files/lma_ICME_2014.pdf">Full Text</a>] 
          </td>
          </tr>
        



          
        <tr>
          <th scope="row">&bull;</th>
          <td>
        <strong>Visual Quality Metric for Perceptual Video Coding</strong><br>
      Long Xu, <strong>Lin Ma</strong>, King Ngi Ngan,  Weisi Lin, and Ying Weng<br>
        IEEE Visual Communications and Image Processing (<strong>VCIP</strong>), 2013. <br>
        [<a href="welcome_files/lma_VCIP_2013.pdf">Full Text</a>]
          </td>
          </tr>




          
        <tr>
          <th scope="row">&bull;</th>
          <td>
       <strong>Packet-layer Model for Quality Assessment of Encrypted Video in IPTV Services</strong><br>
      Qian Zhang, Fan Zhang, and <strong>Lin Ma</strong><br> 
        Asia Pacific Signal and Information Processing Association Annual Summit and Conference (<strong>APSIPA ASC</strong>), 2013. <br>
        [<a href="welcome_files/lma_APSIPA_ASC_2013.pdf">Full Text</a>]
          </td>
          </tr>




          
        <tr>
          <th scope="row">&bull;</th>
          <td>
     <strong>High Quality Image Construction from Multiple Low Quality Copies<br>
      Lin Ma</strong>, Long Xu, Qian Zhang, and King Ngi Ngan<br>
        International Workshop on Multimedia Signal Processing (<strong>MMSP</strong>), 2013.<br>
        [<a href="welcome_files/lma_MMSP_2013.pdf">Full Text</a>] 
          </td>
          </tr>



          
        <tr>
          <th scope="row">&bull;</th>
          <td>

     <strong>Reduced Reference Video Quality Assessment Based on Spatial HVS Mutual  Masking and Temporal Motion Estimation<br>
      Lin Ma</strong>, King Ngi Ngan, and Long  Xu<br>
        IEEE International Conference on Multimedia and Expo (<strong>ICME</strong>) in Multimedia  for Humanity Theme Track, 2013.<br>
        [<a href="welcome_files/lma_ICME_2013.pdf">Full Text</a>]

          </td>
          </tr>



      
        <tr>
          <th scope="row">&bull;</th>
          <td>
            
     <strong>Overview of Quality Assessment for Visual Signals and Newly Emerged Trends<br>
      Lin Ma</strong>, Chenwei Deng, Weisi Lin, and King Ngi Ngan<br>
        International Symposium on Circuits and Systems (<strong>ISCAS</strong>), 2013.<br>
        [<a href="welcome_files/lma_ISCAS_2013.pdf">Full Text</a>]

          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
            
    <strong>Video Quality Metric for Consistent Visual Quality Control in Video Coding</strong><br>
      Long Xu, King Ngi Ngan, Songnan Li, and<strong> Lin Ma</strong><br>
        Asia Pacific Signal and Information Processing Association Annual Summit and Conference (<strong>APSIPA ASC</strong>), 2012. <br>
          [<a href="welcome_files/lma_APSIPA_ASC_2012.pdf">Full Text</a>] 

          </td>
          </tr>


        <tr>
          <th scope="row">&bull;</th>
          <td>
            
    <strong>Study of Subjective and Objective Quality Assessment of Retargeted Images<br>
      Lin Ma</strong>, Weisi Lin, Chenwei Deng, and King Ngi Ngan<br>
        International Symposium on Circuits and Systems (<strong>ISCAS</strong>), 2012.<br>
        [<a href="welcome_files/lma_ISCAS_2012.pdf">Full Text</a>] [<a href="http://ivp.ee.cuhk.edu.hk/projects/demo/retargeting/index.html">Project Homepage</a>] 

          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
            
    <strong>Reduced-Reference Image Quality Assessment via Intra- and Inter-Subband Statistical Characteristics in Reorganized DCT Domain<br>
      Lin Ma</strong>, Songnan Li, and King Ngi Ngan<br>
        Asia Pacific Signal and Information Processing Association Annual Summit and Conference (<strong>APSIPA ASC</strong>), 2011.<br> [<a href="welcome_files/lma_APSIPA_ASC_2011.pdf">Full Text</a>].

          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
            
    <strong>Video Quality Assessment by Decoupling Additive Impairments and Detail losses</strong><br>
          Songnan Li, <strong>Lin Ma</strong>, and King Ngi Ngan<br> 
          International Workshop on Quality of Multimedia Experience(<strong>QoMEX</strong>), 2011.<br>
          <a href="welcome_files/snli_QoMEX_2011.pdf">[Full Text</a>] 

          </td>
          </tr>





        <tr>
          <th scope="row">&bull;</th>
          <td>
            
    <strong>Motion Trajectory Based Visual Saliency for Video Quality Assessment<br>
        Lin Ma</strong>, Songnan Li, and King Ngi Ngan<br> 
          International Conference on Image Processing  (<strong>ICIP</strong>), 2011. <br>
          [<a href="welcome_files/lma_ICIP_2011.pdf">Full Text</a>] 

          </td>
          </tr>




        <tr>
          <th scope="row">&bull;</th>
          <td>
            
    <strong>Perceptual Image Compression via Adaptive Block-Based Super-Resolution Directed  Down-Sampling<br>
        Lin Ma</strong>, Songnan Li, and King Ngi Ngan<br>
        International Symposium on Circuits and Systems  (<strong>ISCAS</strong>), 2011.<br>
        [<a href="welcome_files/lma_ISCAS_2011.pdf">Full Text</a>] 

          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
            
   <strong>Video Quality Assessment Based on Adaptive Block-Size Transform  Just-Noticeable Difference Model<br>
        </strong><strong>Lin Ma</strong>, Fan Zhang, Songnan Li, and King Ngi Ngan<br>
          International Conference on Image Processing  (<strong>ICIP</strong>), 2010. <br>
          [<a href="welcome_files/lma_ICIP_2010.pdf">Full Text</a>]

          </td>
          </tr>



         
        <tr>
          <th scope="row">&bull;</th>
          <td>
            
   <strong>Adaptive Block-Size Transform Based Just-Noticeable Difference Profile for  Videos<br>
        Lin Ma</strong>, and King Ngi Ngan<br>
        International Symposium on Circuits and Systems (<strong>ISCAS</strong>), 2010. <br>
        [<a href="welcome_files/lma_ISCAS_2010.pdf">Full Text</a>] 

          </td>
          </tr>




         
        <tr>
          <th scope="row">&bull;</th>
          <td>
            
   <strong>Adaptive  Block-Size Transform Based Just-Noticeable Difference Profile for Images<br>
        Lin Ma</strong>, and King Ngi Ngan<br>
        Pacific-Rim Conference on Multimedia (<strong>PCM</strong>),  2009.<br>
        [<a href="welcome_files/lma_PCM_2009.pdf">Full Text</a>] 

          </td>
          </tr>



         
        <tr>
          <th scope="row">&bull;</th>
          <td>
            
  <strong>Learning-based Image Restoration for Compressed Image through  Neighboring Embedding<br>
        Lin Ma</strong>, Feng Wu, Debin  Zhao, Wen Gao, and Siwei Ma<br>
         Pacific-Rim Conference on Multimedia (<strong>PCM</strong>), 2008.<br>
        [<a href="welcome_files/lma_PCM_2008.pdf">Full Text</a>] <strong><a href="welcome_files/lma_Best_Pape_Award.jpg">(Best Paper  Award)</a>

          </td>
          </tr>


    
        <tr>
          <th scope="row">&bull;</th>
          <td>
            
 <strong>Three-tiered Network Model for Image Hallucination<br>
        Lin Ma</strong>, Yonghua Zhang, Yan Lu,  Feng Wu, and Debin Zhao<br>
        International Conference of Image Processing (<strong>ICIP</strong>), 2008. <br>
         [<a href="welcome_files/lma_ICIP_2008.pdf">Full Text</a>] 

          </td>
          </tr>


    
        <tr>
          <th scope="row">&bull;</th>
          <td>
            
 <strong>Temporal Inconsistency Measure for Video  Quality Assessment</strong><br>
        Songnan Li, <strong>Lin  Ma</strong>, Fan Zhang, and King Ngi Ngan<br>
        Picture Coding Symposium (<strong>PCS</strong>), 2010. <br>
        [<a href="welcome_files/lma_PCS_2010.pdf">Full Text</a>]

          </td>
          </tr>



        <tr>
          <th scope="row">&bull;</th>
          <td>
            
 <strong>Limitation and Challenges of Image Quality  Measurement</strong><br>
        Fan Zhang, Songnan Li, <strong>Lin Ma</strong>, and King Ngi Ngan<br>
        Visual Communications and Image Processing (<strong>VCIP</strong>), 2010.<br>
        [<a href="welcome_files/lma_VCIP_2010.pdf">Full Text</a>] 

          </td>
          </tr>



              
        <tr>
          <th scope="row">&bull;</th>
          <td>
            
<strong>Universal Steganalysis Based on Statistical Models Using Reorganization of  Block-based DCT Coefficients</strong><br>
        Shaohui Liu, <strong>Lin Ma</strong>, Hongxun Yao, and Debin Zhao<br>
        International Conference on Information Assurance  and Security (<strong>IAS</strong>), 2009. <br>
        [<a href="welcome_files/lma_IAS_2009.pdf">Full Text</a>] 

          </td>
          </tr>







      </tbody>
    </table>
</div>




  
</div>

    </div>

    <!-- Footer -->




  

  
<script src="./Publications_files/jquery.min.js.下载" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="./Publications_files/popper.min.js.下载" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="./Publications_files/bootstrap.min.js.下载" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="./Publications_files/mdb.min.js.下载" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  

  


<!-- Medium Zoom JS -->
<script src="./Publications_files/medium-zoom.min.js.下载" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="./Publications_files/zoom.js.下载"></script>


<!-- Load Common JS -->
<script src="./Publications_files/common.js.下载"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer="" type="text/javascript" id="MathJax-script" src="./Publications_files/tex-mml-chtml.js.下载"></script>
<script defer="" src="./Publications_files/polyfill.min.js.下载"></script>


  








</body><!-- jQuery --></html>